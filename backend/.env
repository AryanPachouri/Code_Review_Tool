# LLM Configuration
LLM_API_KEY=your_api_key_here
LLM_PROVIDER=anthropic  # Options: anthropic, openai, local

# Flask Configuration
FLASK_ENV=development
FLASK_DEBUG=True

# Server Configuration
PORT=5000
HOST=0.0.0.0

# Rate Limiting
RATELIMIT_STORAGE_URL=memory://

# Cache Configuration
CACHE_TTL=3600  # 1 hour in seconds